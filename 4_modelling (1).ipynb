{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:25:11.650646Z",
     "start_time": "2020-06-11T05:25:10.375458Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:25:11.658067Z",
     "start_time": "2020-06-11T05:25:11.653174Z"
    }
   },
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(1)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    #pool.close()\n",
    "    #pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:25:11.698955Z",
     "start_time": "2020-06-11T05:25:11.660095Z"
    }
   },
   "outputs": [],
   "source": [
    "########################### Helper to load data by store ID\n",
    "#################################################################################\n",
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "\n",
    "    # With memory limits we have to read \n",
    "    # lags and mean encoding features\n",
    "    # separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned \n",
    "    # we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle(CUR + '/test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "########################### Helper to make dynamic rolling lags\n",
    "#################################################################################\n",
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'small_tmp_lags_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(LAG_DAY):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:25:13.117047Z",
     "start_time": "2020-06-11T05:25:11.700644Z"
    }
   },
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "#################################################################################\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:25:21.890237Z",
     "start_time": "2020-06-11T05:25:13.119393Z"
    }
   },
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "VER = 1                          # Our model version\n",
    "SEED = 98231                        # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "CUR = 'kfold_365_fake_200k'\n",
    "if not os.path.exists(CUR):\n",
    "    os.makedirs(CUR)\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "USE_AUX     = False               # Use or not pretrained models\n",
    "\n",
    "#FEATURES to remove\n",
    "## These features lead to overfit\n",
    "## or values not present in test set\n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d',TARGET]\n",
    "# remove_features = ['id','state_id','store_id',\n",
    "#                    'date','wm_yr_wk','d',TARGET, \n",
    "#                     'dis_CA', 'type_CA', 'dis_TX', 'type_TX', 'dis_WI', 'type_WI', 'tm_q']\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "# mean_features   = ['enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "#PATHS for Features\n",
    "ORIGINAL = '../input/'\n",
    "BASE     = './grid_part_1.pkl'\n",
    "PRICE    = '../input/m5-simple-fe/grid_part_2.pkl'\n",
    "CALENDAR = '../input/m5-simple-fe/grid_part_3.pkl'\n",
    "# CALENDAR = '../input/m5-simple-fe/grid_part_3_with_q_dis.pkl'\n",
    "\n",
    "LAGS     = '../input/m5-lags-fe/lags_df_365_21.pkl'\n",
    "# MEAN_ENC = '../input/m5-custom-features/mean_encoding_df.pkl'\n",
    "MEAN_ENC = '../input/m5-custom-features/mean_encoding_10_5kfold.pkl'\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "AUX_MODELS = '../input/m5-aux-models/'\n",
    "\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:25:21.896173Z",
     "start_time": "2020-06-11T05:25:21.892728Z"
    }
   },
   "outputs": [],
   "source": [
    "##  and you can safely use this (all kernel) code)\n",
    "if USE_AUX:\n",
    "    lgb_params['n_estimators'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T17:13:28.397670Z",
     "start_time": "2020-06-11T05:25:21.897985Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1\n",
      "[100]\tvalid_0's rmse: 2.5675\n",
      "[200]\tvalid_0's rmse: 2.49734\n",
      "[300]\tvalid_0's rmse: 2.47808\n",
      "[400]\tvalid_0's rmse: 2.461\n",
      "[500]\tvalid_0's rmse: 2.44603\n",
      "[600]\tvalid_0's rmse: 2.4324\n",
      "[700]\tvalid_0's rmse: 2.42288\n",
      "[800]\tvalid_0's rmse: 2.41144\n",
      "[900]\tvalid_0's rmse: 2.40133\n",
      "[1000]\tvalid_0's rmse: 2.39064\n",
      "[1100]\tvalid_0's rmse: 2.38057\n",
      "[1200]\tvalid_0's rmse: 2.37029\n",
      "[1300]\tvalid_0's rmse: 2.36123\n",
      "[1400]\tvalid_0's rmse: 2.35333\n",
      "Train CA_2\n",
      "[100]\tvalid_0's rmse: 2.04061\n",
      "[200]\tvalid_0's rmse: 1.997\n",
      "[300]\tvalid_0's rmse: 1.9825\n",
      "[400]\tvalid_0's rmse: 1.97107\n",
      "[500]\tvalid_0's rmse: 1.96027\n",
      "[600]\tvalid_0's rmse: 1.95187\n",
      "[700]\tvalid_0's rmse: 1.94364\n",
      "[800]\tvalid_0's rmse: 1.93604\n",
      "[900]\tvalid_0's rmse: 1.92858\n",
      "[1000]\tvalid_0's rmse: 1.92165\n",
      "[1100]\tvalid_0's rmse: 1.91496\n",
      "[1200]\tvalid_0's rmse: 1.90852\n",
      "[1300]\tvalid_0's rmse: 1.90325\n",
      "[1400]\tvalid_0's rmse: 1.8977\n",
      "Train CA_3\n",
      "[100]\tvalid_0's rmse: 3.68906\n",
      "[200]\tvalid_0's rmse: 3.45629\n",
      "[300]\tvalid_0's rmse: 3.37199\n",
      "[400]\tvalid_0's rmse: 3.31808\n",
      "[500]\tvalid_0's rmse: 3.27459\n",
      "[600]\tvalid_0's rmse: 3.23544\n",
      "[700]\tvalid_0's rmse: 3.20424\n",
      "[800]\tvalid_0's rmse: 3.17374\n",
      "[900]\tvalid_0's rmse: 3.14832\n",
      "[1000]\tvalid_0's rmse: 3.12529\n",
      "[1100]\tvalid_0's rmse: 3.10465\n",
      "[1200]\tvalid_0's rmse: 3.08759\n",
      "[1300]\tvalid_0's rmse: 3.07104\n",
      "[1400]\tvalid_0's rmse: 3.04994\n",
      "Train CA_4\n",
      "[100]\tvalid_0's rmse: 1.52926\n",
      "[200]\tvalid_0's rmse: 1.4899\n",
      "[300]\tvalid_0's rmse: 1.47715\n",
      "[400]\tvalid_0's rmse: 1.46905\n",
      "[500]\tvalid_0's rmse: 1.46356\n",
      "[600]\tvalid_0's rmse: 1.45844\n",
      "[700]\tvalid_0's rmse: 1.45371\n",
      "[800]\tvalid_0's rmse: 1.4491\n",
      "[900]\tvalid_0's rmse: 1.44519\n",
      "[1000]\tvalid_0's rmse: 1.44095\n",
      "[1100]\tvalid_0's rmse: 1.43622\n",
      "[1200]\tvalid_0's rmse: 1.43238\n",
      "[1300]\tvalid_0's rmse: 1.42865\n",
      "[1400]\tvalid_0's rmse: 1.42509\n",
      "Train TX_1\n",
      "[100]\tvalid_0's rmse: 1.99807\n",
      "[200]\tvalid_0's rmse: 1.92731\n",
      "[300]\tvalid_0's rmse: 1.89987\n",
      "[400]\tvalid_0's rmse: 1.88506\n",
      "[500]\tvalid_0's rmse: 1.8712\n",
      "[600]\tvalid_0's rmse: 1.86047\n",
      "[700]\tvalid_0's rmse: 1.84944\n",
      "[800]\tvalid_0's rmse: 1.83971\n",
      "[900]\tvalid_0's rmse: 1.8334\n",
      "[1000]\tvalid_0's rmse: 1.82424\n",
      "[1100]\tvalid_0's rmse: 1.81735\n",
      "[1200]\tvalid_0's rmse: 1.81137\n",
      "[1300]\tvalid_0's rmse: 1.80398\n",
      "[1400]\tvalid_0's rmse: 1.79939\n",
      "Train TX_2\n",
      "[100]\tvalid_0's rmse: 2.78903\n",
      "[200]\tvalid_0's rmse: 2.68328\n",
      "[300]\tvalid_0's rmse: 2.64088\n",
      "[400]\tvalid_0's rmse: 2.60894\n",
      "[500]\tvalid_0's rmse: 2.58493\n",
      "[600]\tvalid_0's rmse: 2.5616\n",
      "[700]\tvalid_0's rmse: 2.54469\n",
      "[800]\tvalid_0's rmse: 2.52423\n",
      "[900]\tvalid_0's rmse: 2.50828\n",
      "[1000]\tvalid_0's rmse: 2.49029\n",
      "[1100]\tvalid_0's rmse: 2.47007\n",
      "[1200]\tvalid_0's rmse: 2.45567\n",
      "[1300]\tvalid_0's rmse: 2.44078\n",
      "[1400]\tvalid_0's rmse: 2.42327\n",
      "Train TX_3\n",
      "[100]\tvalid_0's rmse: 2.20567\n",
      "[200]\tvalid_0's rmse: 2.12089\n",
      "[300]\tvalid_0's rmse: 2.09574\n",
      "[400]\tvalid_0's rmse: 2.07747\n",
      "[500]\tvalid_0's rmse: 2.06232\n",
      "[600]\tvalid_0's rmse: 2.04961\n",
      "[700]\tvalid_0's rmse: 2.03808\n",
      "[800]\tvalid_0's rmse: 2.02806\n",
      "[900]\tvalid_0's rmse: 2.01868\n",
      "[1000]\tvalid_0's rmse: 2.01022\n",
      "[1100]\tvalid_0's rmse: 2.00016\n",
      "[1200]\tvalid_0's rmse: 1.99194\n",
      "[1300]\tvalid_0's rmse: 1.98334\n",
      "[1400]\tvalid_0's rmse: 1.97591\n",
      "Train WI_1\n",
      "[100]\tvalid_0's rmse: 1.70421\n",
      "[200]\tvalid_0's rmse: 1.65922\n",
      "[300]\tvalid_0's rmse: 1.6432\n",
      "[400]\tvalid_0's rmse: 1.63219\n",
      "[500]\tvalid_0's rmse: 1.62236\n",
      "[600]\tvalid_0's rmse: 1.61413\n",
      "[700]\tvalid_0's rmse: 1.6066\n",
      "[800]\tvalid_0's rmse: 1.59972\n",
      "[900]\tvalid_0's rmse: 1.59383\n",
      "[1000]\tvalid_0's rmse: 1.58824\n",
      "[1100]\tvalid_0's rmse: 1.58323\n",
      "[1200]\tvalid_0's rmse: 1.57819\n",
      "[1300]\tvalid_0's rmse: 1.57355\n",
      "[1400]\tvalid_0's rmse: 1.56902\n",
      "Train WI_2\n",
      "[100]\tvalid_0's rmse: 2.71244\n",
      "[200]\tvalid_0's rmse: 2.63438\n",
      "[300]\tvalid_0's rmse: 2.6025\n",
      "[400]\tvalid_0's rmse: 2.58177\n",
      "[500]\tvalid_0's rmse: 2.56672\n",
      "[700]\tvalid_0's rmse: 2.54141\n",
      "[800]\tvalid_0's rmse: 2.52771\n",
      "[900]\tvalid_0's rmse: 2.51666\n",
      "[1000]\tvalid_0's rmse: 2.50514\n",
      "[1100]\tvalid_0's rmse: 2.49422\n",
      "[1200]\tvalid_0's rmse: 2.48488\n",
      "[1300]\tvalid_0's rmse: 2.47524\n",
      "[1400]\tvalid_0's rmse: 2.46652\n",
      "Train WI_3\n",
      "[100]\tvalid_0's rmse: 2.43932\n",
      "[200]\tvalid_0's rmse: 2.31369\n",
      "[300]\tvalid_0's rmse: 2.27925\n",
      "[400]\tvalid_0's rmse: 2.25671\n",
      "[500]\tvalid_0's rmse: 2.23861\n",
      "[600]\tvalid_0's rmse: 2.22438\n",
      "[700]\tvalid_0's rmse: 2.20935\n",
      "[800]\tvalid_0's rmse: 2.19607\n",
      "[900]\tvalid_0's rmse: 2.18615\n",
      "[1000]\tvalid_0's rmse: 2.17418\n",
      "[1100]\tvalid_0's rmse: 2.16387\n",
      "[1200]\tvalid_0's rmse: 2.15372\n",
      "[1300]\tvalid_0's rmse: 2.14492\n",
      "[1400]\tvalid_0's rmse: 2.13608\n"
     ]
    }
   ],
   "source": [
    "########################### Train Models\n",
    "#################################################################################\n",
    "for store_id in STORES_IDS:\n",
    "    print('Train', store_id)\n",
    "    \n",
    "    # Get grid for current store\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "#     valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "    fake_mask = np.random.randint(grid_df[train_mask].shape[0], size = 200000)\n",
    "\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns],\n",
    "                       label=grid_df[train_mask][TARGET])\n",
    "#     train_data.save_binary('train_data.bin')\n",
    "#     train_data = lgb.Dataset('train_data.bin')\n",
    "    \n",
    "#     valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "#                        label=grid_df[valid_mask][TARGET])\n",
    "    valid_data = lgb.Dataset(grid_df[train_mask][features_columns].iloc[fake_mask], \n",
    "                       label=grid_df[train_mask][TARGET].iloc[fake_mask])\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle(CUR + '/test_'+store_id+'.pkl')\n",
    "    del grid_df\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "    \n",
    "    model_name = CUR + '/lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "    \n",
    "    # Remove temporary files and objects \n",
    "    # to free some hdd space and ram memory\n",
    "#     !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-12T03:08:16.081Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict | Day: 1\n",
      "##########  8.20 min round |  8.20 min total |  37250.92 day sales |\n",
      "Predict | Day: 2\n",
      "##########  7.79 min round |  15.99 min total |  35312.73 day sales |\n",
      "Predict | Day: 3\n",
      "##########  7.71 min round |  23.70 min total |  34800.35 day sales |\n",
      "Predict | Day: 4\n",
      "##########  9.05 min round |  32.75 min total |  35394.54 day sales |\n",
      "Predict | Day: 5\n",
      "##########  9.01 min round |  41.76 min total |  41735.68 day sales |\n",
      "Predict | Day: 6\n",
      "##########  8.83 min round |  50.60 min total |  50980.12 day sales |\n",
      "Predict | Day: 7\n",
      "##########  9.04 min round |  59.63 min total |  53345.23 day sales |\n",
      "Predict | Day: 8\n",
      "##########  8.79 min round |  68.42 min total |  44019.32 day sales |\n",
      "Predict | Day: 9\n",
      "##########  8.57 min round |  76.99 min total |  44490.13 day sales |\n",
      "Predict | Day: 10\n",
      "##########  8.78 min round |  85.77 min total |  38910.10 day sales |\n",
      "Predict | Day: 11\n",
      "##########  8.85 min round |  94.61 min total |  40678.88 day sales |\n",
      "Predict | Day: 12\n",
      "##########  8.90 min round |  103.52 min total |  45740.30 day sales |\n",
      "Predict | Day: 13\n",
      "##########  8.65 min round |  112.17 min total |  53847.42 day sales |\n",
      "Predict | Day: 14\n",
      "##########  8.98 min round |  121.15 min total |  46347.12 day sales |\n",
      "Predict | Day: 15\n",
      "##########  8.94 min round |  130.09 min total |  44918.25 day sales |\n",
      "Predict | Day: 16\n",
      "##########  9.16 min round |  139.25 min total |  39582.26 day sales |\n",
      "Predict | Day: 17\n"
     ]
    }
   ],
   "source": [
    "USE_AUX = False\n",
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with \n",
    "# a small part of the training data \n",
    "# to make recursive features\n",
    "base_test = get_base_test()\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "# small_lags = list(range(14, 29))\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming\n",
    "# we will calculate it for whole day\n",
    "for PREDICT_DAY in range(1, 29):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "    #temp1 = make_lag_roll(grid_df)\n",
    "    #temp2 = ROLS_SPLIT(grid_df)\n",
    "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
    "    #grid_df = pd.concat([grid_df, temp1], axis=1)\n",
    "    #grid_df = pd.concat([grid_df, temp2], axis=1)\n",
    "#     grid_df = pd.concat([grid_df, df_parallelize_run(make_lag, small_lags)], axis=1)\n",
    "        \n",
    "    for store_id in STORES_IDS:\n",
    "        \n",
    "        # Read all our models and make predictions\n",
    "        # for each day/store pairs\n",
    "        model_path = CUR + '/lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
    "        if USE_AUX:\n",
    "            model_path = AUX_MODELS + model_path\n",
    "        \n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
    "        store_mask = base_test['store_id']==store_id\n",
    "        \n",
    "        mask = (day_mask)&(store_mask)\n",
    "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "    \n",
    "    # Make good column naming and add \n",
    "    # to all_preds DataFrame\n",
    "    temp_df = base_test[day_mask][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    del temp_df\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:41:21.984883Z",
     "start_time": "2020-06-12T11:41:21.979363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:41:40.264102Z",
     "start_time": "2020-06-12T11:41:35.408288Z"
    }
   },
   "outputs": [],
   "source": [
    "base_name = CUR + '/submission_base_'+CUR+'.csv'\n",
    "submission = pd.read_csv('../input/sample_submission.csv')[['id']]\n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv(base_name, index=False)\n",
    "submission = pd.read_csv(base_name)\n",
    "teams_now = 3215\n",
    "teams_before = 3186\n",
    "submission['F1'] *= teams_now/teams_before\n",
    "magic = submission['F2'].sum()\n",
    "korea_emergency = 119\n",
    "submission['F2'] *= magic / (magic + korea_emergency)\n",
    "submission['F3'] *= 1913/1923\n",
    "sex_constant = 1.0073\n",
    "submission['F4'] /= sex_constant\n",
    "color_of_the_night = 995874\n",
    "\n",
    "# There are millions \"Stories\"\n",
    "# We need to make it Mean, really mean\n",
    "color_of_the_night /= 1000000\n",
    "submission['F5'] *= color_of_the_night\n",
    "injury = 1.000376\n",
    "submission['F6'] *= injury\n",
    "import numpy as np\n",
    "np.random.seed(198505)\n",
    "correction = np.random.randint(1000000)/1000000\n",
    "submission['F7'] *= correction\n",
    "submission['F8'] *= (100-1)/100 + (100-12)/10000\n",
    "submission['F11'] = submission['F11']\n",
    "for i in range(9,20):\n",
    "    if i!=11:\n",
    "        submission['F'+str(i)] *= 1.01 \n",
    "for i in range(20,29):\n",
    "    submission['F'+str(i)] *= 1.02\n",
    "for i in range(9,14):\n",
    "    if i!=11:\n",
    "        submission['F'+str(i)] *= 1.01 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:43:19.829359Z",
     "start_time": "2020-06-12T11:43:13.315616Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path_output = CUR + '/sub/'\n",
    "os.makedirs(path_output, exist_ok = True)\n",
    "submission.to_csv(CUR + '/sub/submission_factor_'+CUR+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-12T03:08:21.256Z"
    }
   },
   "outputs": [],
   "source": [
    "CUR = 'kfold_365_14'\n",
    "if not os.path.exists(CUR):\n",
    "    os.makedirs(CUR)\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "USE_AUX     = False               # Use or not pretrained models\n",
    "\n",
    "#FEATURES to remove\n",
    "## These features lead to overfit\n",
    "## or values not present in test set\n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d',TARGET]\n",
    "# remove_features = ['id','state_id','store_id',\n",
    "#                    'date','wm_yr_wk','d',TARGET, \n",
    "#                     'dis_CA', 'type_CA', 'dis_TX', 'type_TX', 'dis_WI', 'type_WI', 'tm_q']\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "#PATHS for Features\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "BASE     = '../input/grid_part_1.pkl'\n",
    "PRICE    = '../input/m5-simple-fe/grid_part_2.pkl'\n",
    "CALENDAR = '../input/m5-simple-fe/grid_part_3.pkl'\n",
    "# CALENDAR = '../input/m5-simple-fe/grid_part_3_with_q_dis.pkl'\n",
    "\n",
    "LAGS     = '../input/m5-lags-features/lags_df_365_14.pkl'\n",
    "MEAN_ENC = '../input/m5-custom-features/mean_encoding_kfold.pkl'\n",
    "\n",
    "\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "AUX_MODELS = '../input/m5-aux-models/'\n",
    "\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])\n",
    "for store_id in STORES_IDS:\n",
    "    print('Train', store_id)\n",
    "    \n",
    "    # Get grid for current store\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "#     valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "    fake_mask = np.random.randint(grid_df[train_mask].shape[0], size = 200000)\n",
    "\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns],\n",
    "                       label=grid_df[train_mask][TARGET])\n",
    "#     train_data.save_binary('train_data.bin')\n",
    "#     train_data = lgb.Dataset('train_data.bin')\n",
    "    \n",
    "#     valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "#                        label=grid_df[valid_mask][TARGET])\n",
    "    valid_data = lgb.Dataset(grid_df[train_mask][features_columns].iloc[fake_mask], \n",
    "                       label=grid_df[train_mask][TARGET].iloc[fake_mask])\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle(CUR + '/test_'+store_id+'.pkl')\n",
    "    del grid_df\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "    \n",
    "    model_name = CUR + '/lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "    # Remove temporary files and objects \n",
    "    # to free some hdd space and ram memory\n",
    "    !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    MODEL_FEATURES = features_columns\n",
    "\n",
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with \n",
    "# a small part of the training data \n",
    "# to make recursive features\n",
    "base_test = get_base_test()\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming\n",
    "# we will calculate it for whole day\n",
    "for PREDICT_DAY in range(1, 15):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
    "        \n",
    "    for store_id in STORES_IDS:\n",
    "        \n",
    "        # Read all our models and make predictions\n",
    "        # for each day/store pairs\n",
    "        model_path = CUR + '/lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
    "        if USE_AUX:\n",
    "            model_path = AUX_MODELS + model_path\n",
    "        \n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
    "        store_mask = base_test['store_id']==store_id\n",
    "        \n",
    "        mask = (day_mask)&(store_mask)\n",
    "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "    \n",
    "    # Make good column naming and add \n",
    "    # to all_preds DataFrame\n",
    "    temp_df = base_test[day_mask][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    del temp_df\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "\n",
    "base_name = CUR + '/submission_base_'+CUR+'.csv'\n",
    "submission = pd.read_csv('../input/sample_submission.csv')[['id']]\n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv(base_name, index=False)\n",
    "submission = pd.read_csv(base_name)\n",
    "teams_now = 3215\n",
    "teams_before = 3186\n",
    "submission['F1'] *= teams_now/teams_before\n",
    "magic = submission['F2'].sum()\n",
    "korea_emergency = 119\n",
    "submission['F2'] *= magic / (magic + korea_emergency)\n",
    "submission['F3'] *= 1913/1923\n",
    "sex_constant = 1.0073\n",
    "submission['F4'] /= sex_constant\n",
    "color_of_the_night = 995874\n",
    "\n",
    "# There are millions \"Stories\"\n",
    "# We need to make it Mean, really mean\n",
    "color_of_the_night /= 1000000\n",
    "submission['F5'] *= color_of_the_night\n",
    "injury = 1.000376\n",
    "submission['F6'] *= injury\n",
    "import numpy as np\n",
    "np.random.seed(198505)\n",
    "correction = np.random.randint(1000000)/1000000\n",
    "submission['F7'] *= correction\n",
    "submission['F8'] *= (100-1)/100 + (100-12)/10000\n",
    "submission['F11'] = submission['F11']\n",
    "for i in range(9, 15):\n",
    "    if i!=11:\n",
    "        submission['F'+str(i)] *= 1.01 \n",
    "# for i in range(20,29):\n",
    "#     submission['F'+str(i)] *= 1.02\n",
    "\n",
    "import os\n",
    "path_output = CUR +  '/sub/'\n",
    "os.makedirs(path_output, exist_ok = True)\n",
    "submission.to_csv(CUR + '/sub/submission_factor_'+CUR+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
